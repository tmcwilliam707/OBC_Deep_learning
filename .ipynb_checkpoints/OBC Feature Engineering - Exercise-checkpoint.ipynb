{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "At this point, it's assumed that you've explored the [battery charging dataset on Trove](https://trove.apple.com/dataset/aiedu_battery_charging/1.0.0) and are familiar with the variables in that data and what they represent. \n",
    "\n",
    "In this notebook, it will be up to you to transform the battery charging dataset into a dataset that can be used for training an ML model, including calculating and adding new columns to the data that can be used as input features. \n",
    "\n",
    "> This is a step known as **feature engineering**, where input features are numerical values or categories that will help you to make accurate predictions.\n",
    "\n",
    "Format-wise your final data should satisfy the following requirements:\n",
    "* You've created at least 2 input features that you think will be useful for prediction\n",
    "* Every column is an input feature or a predictive target (targets are typically the _last_ column(s) in a dataset)\n",
    "* There are no missing (nan) values in your data\n",
    "* Your data is split into two sets: training and test\n",
    "\n",
    "**Your final training and test datasets should be something that can be directly used for training and evaluating a simple NN or a baseline model.**\n",
    "\n",
    "## Steps for creating features\n",
    "\n",
    "In the rest of this notebook, you will be tasked with performing the following steps and creating an _initial_ featurized dataset to use for model training: \n",
    ">1. **Loading the data**: Create an initial DataFrame to work with \n",
    "2. **Define a target**: Define a predictive target and make it one column in your data\n",
    "3. **Removing noise and null values**: Removing values that are noisy or not useful for the predictive model you want to build\n",
    "4. **Train/test split**: Splitting the data into train/test sets\n",
    "5. **Creating features**: Transforming data and creating new input features that you think will have predictive power\n",
    "6. **Saving data**: Saving your transformed training and test data so you can use it for model training!\n",
    "\n",
    "Don't worry about making the _best_ features; consider this notebook a starting point that you can revisit; later, adding new features and removing others to get the best features for your predictive goals. \n",
    "\n",
    "Once you've completed this notebook, you'll submit it, alongside any helper functions, for review by the Education team. \n",
    "\n",
    ">Your tasks will be marked as **TASK**s in markdown and `##TODOs` in code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turitrove as trove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load (or Mount) the Data\n",
    "\n",
    "In the last EDA notebook, you might have saved some transformed battery charging data in a binary file (pickle) format. You can either load that saved data, with with pandas `.read_pickle()` or `read_csv()` or start anew by mounting the Trove battery dataset as usual.\n",
    "\n",
    "> **TASK**: Load in the battery charging data as a DataFrame. And, make sure your date-time stamps are formatted correctly; not as generic object types but as date-time stamps. \n",
    "\n",
    "Recall, you can find the relevant info for mounting a Trove dataset on it's [Trove page](https://trove.apple.com/dataset/aiedu_battery_charging/1.0.0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Load or mount the battery data as a DataFrame\n",
    "\n",
    "## TODO: Make sure the date-time variables are formatted as date-time stamps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define a Target Column\n",
    "\n",
    "Since you want to predict the duration that a device will be _plugged in_, you're going to want to **create a target variable** that captures that information.  \n",
    "\n",
    "If you loaded in your own, explored data, you may have already completed this step. If not, make sure to complete this task:\n",
    "\n",
    ">**TASK**: Create a DataFrame that represents _only_ when users have plugged-in their devices and the devices are charging, and add a column to the DataFrame that represents the `duration` (in minutes) of these plugged-in events.\n",
    "\n",
    "**Hints**: \n",
    "* Make sure your data is in the correct format for making certain calculations, e.g. with timestamps.\n",
    "* You can get specific rows of data by using [pandas conditional filtering](https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/03_subset_data.html#how-do-i-filter-specific-rows-from-a-dataframe)\n",
    "* Use `.copy()`: When making new DataFrames, it's good practice to name them something uniqe as you modify them _and_ to use the `.copy()` function to make sure you're making a new copy rather than just a reference to the original DataFrame.  See [the Python docs on copy](https://docs.python.org/3/library/copy.html) for more details about that nuance.\n",
    "\n",
    "After this task is complete, your plug-in events dataset should look a bit like this (these are just a few fake, example rows—with calendar dates removed—which won't _exactly_ match your dataset, except for the stream and value columns):\n",
    "```\n",
    "start\t   end\t      stream\t            value  user_id\tduration\n",
    "\n",
    "04:53:00\t05:32:00\t/device/isPluggedIn\t1.0\tagh-184\t39.0\n",
    "07:20:00\t16:59:00\t/device/isPluggedIn\t1.0\tagh-184\t579.0\n",
    "07:30:00\t07:30:00\t/device/isPluggedIn\t1.0\tagh-184\t0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Create a dataframe of plug-in events only (using .copy())\n",
    "\n",
    "## TODO: Add a 'duration' column that represents the length of a charge in mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cells\n",
    "\n",
    "In this notebook, you'll find a few provided test cells; you should pass in your _current_, working DataFrame and these tests will check whether your data format looks about right. These tests are not extensive, they are just meant to tell you whether you are on the right track to proceed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- TEST CELL --- ##\n",
    "## TODO: replace None function arg with your DataFrame from the above exercise\n",
    "\n",
    "from checks import test_dtypes_plugin_vals\n",
    "    \n",
    "test_dtypes_plugin_vals(None) ## YOUR DF HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What about the other rows of data?\n",
    "\n",
    "For optimized battery charging, we want to predict how long someone is likely to plug-in and charge their device, and so the plug-in events are the most important source of data, but you can still use information about un-plugged events and battery charge levels to create input features that may be useful for prediction. \n",
    "\n",
    "This information can be _merged_ into your plug-in events DataFrame. \n",
    "\n",
    "### Merging data: example\n",
    "\n",
    "For example, say you want to record the battery charge level at the `start` of a plug-in event, because you hypothesize that when users have a lower charge level, they are more likely to plug-in their devices for a long `duration`. \n",
    "\n",
    "> However, plug-in events and battery charge level information are contained in two different _streams_ of data: '/device/isPluggedIn' and '/device/batteryPercentage'.\n",
    "\n",
    "Since we're interested in the `start` charge level at the start of a plug-in event, we can do what's called a data **merge** or join on that column by doing the following two steps. \n",
    "1. Create two different DataFrames; one for plug-in events and one for battery charge levels\n",
    "2. Merge the two based on their respective `start` times to create a single DataFrame with _both_ plug-in and battery charge level information. \n",
    "\n",
    "### Not-matching `start` times\n",
    "\n",
    "Pandas provides a few different functions for merging DataFrames, including `merge()` which is helpful for merging on exactly-matching values, but this is not useful in this case because the `start` of a battery charge level event and the `start` of a plug-in event may not match up to the minute. \n",
    "\n",
    "Instead we want a merge function that allows us to match on the _closest_ `start` times between the two streams, which we can do with the pandas function `merge_asof()`. \n",
    "\n",
    "**Resources**: \n",
    "* The [pandas documentation for `merge_asof()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge_asof.html)\n",
    "* [This blog post](https://towardsdatascience.com/how-to-merge-not-matching-time-series-with-pandas-7993fcbce063) on merging not-exactly-matching data with `merge_asof()`. \n",
    "\n",
    "> **TASK**: Create a DataFrame of _only_ the battery level stream; you should already have a DF of plug-in events from earlier tasks. Define those two DataFrames, then run the provided code to use `merge_asof()` and get a `battery_start_level` column in a new DataFrame, `plug_in_charge_df`.\n",
    "\n",
    "> The resultant, `plug_in_charge_df` should be the DataFrame you continue to work with, as you proceed in this notebook. \n",
    "\n",
    "In the provided code, the two DataFrames that you provide will be merged based on a few parameters:\n",
    "* on - column name that I want to *near-match* in the left and right df's (`start`)\n",
    "* by - column(s) that should *exactly-match* between left and right df's (`user_id`)\n",
    "* direction - whether to search for prior, next, or generally closest matches for the _on_ column values (`forward`)\n",
    "\n",
    "Combined, these parameters ensure that we get the first recorded battery percentage `value` that _starts_ immediately after the `start` of a given plugged-in event, for a given `user_id`.\n",
    "\n",
    "**Note**: Merging _can_ introduce some missing values into your data, if a nearest start time is not found upon merging. You will be asked to check for these missing values and deal with them in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define two dataframes for plug-in and battery charge level streams\n",
    "\n",
    "plug_in_df = None ## your plug-in events DF, created earlier ##\n",
    "battery_charge_df = None  ## create a battery charge level DF ## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: run provided code\n",
    "\n",
    "### --- provided code --- ### \n",
    "\n",
    "# merge two df's on closest start time + matching user_id's\n",
    "# sorting df's on `start`\n",
    "plug_in_charge_df = pd.merge_asof(plug_in_df.sort_values('start'),         ## left df\n",
    "                                  battery_charge_df.sort_values('start'),  ## right df\n",
    "                                  on='start',\n",
    "                                  by = 'user_id',                          ## id's should match\n",
    "                                  direction = 'forward',                   ## right df's start is >= left start\n",
    "                                  suffixes=('_plugin', '_batt_level'))                   \n",
    "\n",
    "# view resultant df\n",
    "plug_in_charge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns \n",
    "\n",
    "At this point, you should see quite a bit of information in the `plug_in_charge_df` DataFrame. There are two columns for the separate streams of data, end times, and values associated with each.\n",
    "\n",
    "But recall: we just wanted to add the starting battery charge level to your plug-in events DataFrame; this is currently represented as the `value_batt_level` column.\n",
    "\n",
    "So, next, I provide some code for dropping some extraneous battery level information, and re-naming the `value_batt_level` column to `battery_start_level`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- provided code --- ### \n",
    "\n",
    "# drop some battery_level columns\n",
    "plug_in_charge_df = plug_in_charge_df.drop(['end_batt_level', 'stream_batt_level'], axis=1)\n",
    "plug_in_charge_df = plug_in_charge_df.rename(columns={\"end_plugin\": \"end\", \n",
    "                                                      \"stream_plugin\": \"stream\",\n",
    "                                                      \"value_plugin\": \"value\", \n",
    "                                                      \"value_batt_level\": \"start_batt_level\"})\n",
    "# see result\n",
    "plug_in_charge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your work\n",
    "\n",
    "At this point, a sample of your data should look something like this; same as your initial plug-in events frame but with a `duration` and `start_batt_level` column. \n",
    "\n",
    "```\n",
    "start\t end\t   stream\t         value  user_id\tduration start_batt_level\n",
    "\n",
    "04:53:00  05:32:00  /device/isPluggedIn  1.0  agh-184\t39.0     74.0\n",
    "07:20:00  16:59:00  /device/isPluggedIn  1.0  agh-184\t579.0    9.0\n",
    "07:30:00  07:30:00  /device/isPluggedIn  1.0  agh-184\t0.0      81.0\n",
    "```\n",
    "\n",
    "In addition to checking that you have these columns, here is also a good point to **explore** (again) and check:\n",
    "1. That the format of your data is expected; numbers are likely float values and dates are date-time's\n",
    "2. If you've introduced any missing values with the merge step—remember, you'll want to create data without any missing values for training and ML model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check your work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Deal with Noisy Data\n",
    "\n",
    "Noise is data that is irrelevant or incorrect information for a task.\n",
    "\n",
    "There is some incorrect noise in this data, which you may have noticed in earlier explorations, caused by a somewhat noisy data collection pipeline:\n",
    "* Occasionally date-times will be incorrectly marked in the far future (e.g. in the year 2170)\n",
    "\n",
    "There is also some noise with respect to the Optimized Battery Charging feature which is designed to work for predicting plug-in charge durations that are longer than 0 minutes but shorter than 2 days. \n",
    "* 0-length charges are typically \"blips\" that may be caused by a wireless charger breaking contact with a device momentarily, and\n",
    "* Greater than 48hr charge-lengths will be handled by a rule-based algorithm that pauses charging at 80% indefinitely.\n",
    "\n",
    "Now, it is up to you to remove this noise.\n",
    "> **TASK**: Remove noisy data in the `plug_in_charge_df`.\n",
    "> 1. Remove any rows of data that contain start/end times that were collected past this year (2021)—data could _not_ have been collected in the future, so this is noise that should be removed.\n",
    "> 2. Remove any plug-in `duration` values that represent charge \"blips\" or durations that are recorded as 0-length; 0-length charges are too short to engage or benefit from optimized battery charging. \n",
    "> 3. Remove any `duration` values that are greater than or equal to 48hrs in length, which will be routed to a different algorithm for pausing charging. \n",
    ">4. Identify and decide whether to remove or replace/impute NaN values in your data.\n",
    "\n",
    "At the end of this section, you should be left with a DataFrame without any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Remove noisy data and remove/replace missing data\n",
    "# No dates past 2021\n",
    "# No 0-length or >= 48hr charge lengths\n",
    "# No missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cell\n",
    "\n",
    "Run the following tests to see if you're on the right track and have a DataFrame without any missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- TEST cell, replace None with your df --- ##\n",
    "from checks import noise_tests\n",
    "\n",
    "noise_tests(None) ## YOUR DF HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you _almost_ have data that is ready for training a very simple ML model!\n",
    "\n",
    "You should have one, potentially-useful input feature: `start_batt_level` and one target, charge `duration` (in minutes). \n",
    "\n",
    "The final steps are to: \n",
    "* Split this data for training _and_ testing an ML model\n",
    "* Add more input features of your own design\n",
    "* Save the datasets in a well-structured form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train/Test Split\n",
    "\n",
    "Now that you have de-noised (or \"cleaned\") your data, next, you'll need to create separate training and test datasets for building and evaluating any ML model you create. \n",
    "\n",
    "For time-series data, it is recommended that you do this split in _time_ rather than just randomly selecting data points to create these two datasets. \n",
    "\n",
    ">The below code does this for you, as long as you provide your de-noised dataset with a `user_id` column and a `start` column.\n",
    "\n",
    "This code uses a provided helper function that takes in a DataFrame of plug-in events, sorts it by `start` time and splits the data for each user into about 80/20 train and test data. The function returns a train and test DataFrame with the same columns as in the input DataFrame. You are welcome to take a look at this function in `helpers.py`, if you like, but you are not expected to modify it. \n",
    "\n",
    "It may take **up to 10mins to run**, depending on your machine and size of the df you pass in. It is suggested that you take a break or read up on some useful references in this time:\n",
    "* This wikipedia page describes why we split in time to avoid [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning))\n",
    "* Kaggle has several pandas-related learning resources including [this short lesson](https://www.kaggle.com/ryanholbrook/creating-features) on pandas and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- provided code --- ### \n",
    "# Replace None with the name of your clean, plug events dataframe, and run this code\n",
    "\n",
    "from helpers import split80_20\n",
    "train_df, test_df = split80_20(None) ## YOUR DF HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- provided code --- ### \n",
    "# Check that the train/test sizes are about 80/20\n",
    "\n",
    "print('Train length: ', len(train_df))\n",
    "print('Test length: ', len(test_df))\n",
    "print()\n",
    "print('Decimal % test data of total data: ', len(test_df)/(len(test_df)+len(train_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create (More) Features\n",
    "\n",
    "At this point, you should have training and test sets of data that contain quite a few columns, including a target charge `duration` for an ML model to predict, and the battery level at the start of a plug-in event, `start_batt_level`, which will act as one input feature.\n",
    "\n",
    "> This is a good point to save your progress by **saving your current train/test datasets** in case you want to re-load them or modify them later on. You'll also be prompted to save this data in a _specific_ format for training an ML model, at the end of this notebook.\n",
    "\n",
    "**To complete this notebook, you are required to create at least 1 more input feature of your own.**\n",
    "\n",
    "These features should represent information you think will be useful in making a prediction about charge duration. There is only one rule to heed as you create any time-dependent features:\n",
    "> The data used in prediction (test data) must be known at the time a prediction is to be made.\n",
    "\n",
    "What does this mean?  \n",
    "* You can't create any features in the test data, `test_df` that aren't known at the time of testing or before.\n",
    "* Typically this means calculating any statistical input features, like the mean plug-in duration for a user around every hour, using _training_ data, `train_df` that was recorded in the past.\n",
    "\n",
    ">**TASK**: Create _at least_ one more column that represents an engineered feature; this column should be added to both the train and test datasets.\n",
    "\n",
    "You may find it useful to refer back to the example notebook, **Feature Engineering for Household Power** in which statistical features, `Mean_hourly_power` and `Std_hourly_power`, are calculated from _only_ the training data and added as a column to _both_ train and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Create 1+ more input features for the train_df and test_df\n",
    "\n",
    "## TODO: (Optional) Save your working train and test sets to revisit later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Save Your Data\n",
    "\n",
    "Before you use your data to train a model, it is convention to save it in a specific format:\n",
    "> **TASK**: Format your final training and test datasets for model training and eval, and save them.\n",
    "> 1. Your target variable should be the _last_ column in the dataset\n",
    "> 2. All other columns should be different input features (other information should be _dropped_).\n",
    "> 3. Save your training and test datasets as two, csv or (recommended) binary pickle files for later loading.\n",
    "\n",
    "**ALSO**: Un-mount any Trove data you are no longer using; you'll likely used your locally saved data for model training, henceforth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Format train/test data: all cols are features, last col is the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Save the train and test sets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Unmount Trove data, if you no longer need it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Going further and iteration\n",
    "At this point, you should have initial, featurized datasets to start training and evaluating ML models. Great work! \n",
    "\n",
    "> Training and evaluating a baseline is exactly what you'll do in the next notebook. \n",
    "\n",
    "You can even add to this notebook and train your first, simple model! Especially if you are familiar with the ML libraries, Turicreate or scikit-learn, you can simply pass in your training data into a simple regression model and evluate on the test dataset.\n",
    "\n",
    "You may end up returning to this notebook or creating a new one to explore creating other features—you are left to your imagination about what will and won't work for this use case and you are encouraged to use your peers and instructors as brainstorming partners!  \n",
    "\n",
    "Feature engineering is very open-ended, full of exploration, and marked by trial and error. If you can think of a feature your dataset might provide that would be useful in predicting the target value, create it. Then test it out by fitting a model to it and evaluating the fit.  \n",
    "\n",
    "If you're uncertain where to start, go back to the instructional modules in Canvas. There are ideas for creating different types of features there. Then think about the data you have, and what types of features you might create. There's no \"correct\" answer, only what you can dream up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
