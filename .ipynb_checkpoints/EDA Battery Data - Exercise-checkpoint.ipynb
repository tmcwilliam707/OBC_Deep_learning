{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Exploratory Data Analysis or EDA is to **gain an understanding of your data**. \n",
    "\n",
    "Upon first getting some data, you'll want to know more about what each variable represents and how the data is formatted. You may want to graph and visualize your data to find any patterns that might be lurking within it. You also want to look for any missing data, see how prevalent it is, and decide what to do about it.  \n",
    "\n",
    "Each section below will walk you through what to do in order to begin understanding your data, and you'll complete the following steps:\n",
    "* Mount data from Trove (again)\n",
    "* Answer Andy's (and ask your own) questions about the data \n",
    "\n",
    "You'll see tasks for you to complete tagged as **TASK** in markdown and `##TODOs` in code. \n",
    "\n",
    "Let's get started!\n",
    "\n",
    "> **First TASK**: Write a couple import statements for standard libraries, pandas and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turitrove as trove\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "## TODO: Write additional import statements for pandas (as pd) and numpy (as np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Mount the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need data before you can look at it. Your code in this section should mount a battery charging dataset object from Turi Trove on your computer, and load the data file into this notebook.  \n",
    "\n",
    ">**TASK**: Mount the [aiedu_battery_charging dataset](https://trove.apple.com/dataset/aiedu_battery_charging/1.0.0) from Trove, and read it in as a pandas DataFrame. \n",
    "\n",
    "Depending on a few things outside of your control, reading the provided file in as a DataFrame could take 30 seconds or a few minutes; it is a lot of data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provided un-mount step \n",
    "trove.umount('dataset/aiedu_battery_charging@1.0.0')\n",
    "\n",
    "## TODO: Mount the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define a path for finding the specific raw data file\n",
    "\n",
    "## TODO: Read in the path as a pandas dataframe or turicreate SFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code ran without any errors, congrats, you should have a DataFrame to explore and work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initial Explorations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you want to do is get eyes on your data, see how big your dataset is, and start getting a feel for it. You'll do that in these first few cells.  \n",
    "\n",
    "> **TASK**: Use `.shape` and `.head()` to get the very first understandings of your data: how big is it, and what does it look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Print the shape and a few rows of the df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TASK**: Check the data formats, using the `.dtypes` attribute for a DataFrame. Look for types that don't seem to match up with what you see in each data column.  \n",
    "\n",
    "Examples of unexpected types would be:\n",
    "* Data that appears to be numbers (like float values or integers) but is *typed* as a generic object or string variable  \n",
    "* Date-time values that are typed as objects or strings (instead of as date-time stamps)\n",
    "\n",
    "These are some of most common unexpected types, but almost anything can happen with data, so note anything that you have questions about.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Identify the type of data in each column, what seems right or wrong?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reformat your data into the types that you want and expect, `pandas` offers several conversion functions, such as `to_datetime()` and `to_numeric()`. You can find common conversions in the [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html). \n",
    "\n",
    "One such conversion is provided for you, in the code cell below. \n",
    "\n",
    "You can also choose to *re-read* in your data with `pd.read_csv()` where you can specify _how_ you want individual columns read-in as. To see an example of this, please refer to the **EDA for Household Power Prediction** notebook.\n",
    "\n",
    "### Convert to work with numeric values and date-times\n",
    "\n",
    "For the explorations and transformations you'll need to do, it is important that the `start` and `end` dates be date time objects and that the `value` column be a numeric type like a float value. We won't need to do string operations on the `stream` or `user_id` columns, so those can be generic objects or string variables. \n",
    "\n",
    "> **TASK**: Convert or read in the data such that dates and numeric values are correctly typed. After conversion, you can check your work with another call to the `.dtypes` attribute for a DataFrame. This should print out something similar to the following:\n",
    "\n",
    "```\n",
    "start      datetime64[ns]\n",
    "end        datetime64[ns]\n",
    "stream             object\n",
    "value             float64\n",
    "user_id            object\n",
    "dtype: object\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes your DataFrame is named df and converts the start column to a date-time stamp\n",
    "df['start']= pd.to_datetime(df['start'])\n",
    "\n",
    "## TODO: convert other numeric columns to be the type you expect\n",
    "## TODO: Check your work by called df.dtypes again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Forming & Answering Questions\n",
    "\n",
    "It is easier to create code to answer **low-level** questions, which are very specific and usually framed in _terms_ of the dataset itself, e.g., in terms of `start` and `end` times or `value` ranges for the battery data.\n",
    "\n",
    "This section is meant to encourage your creativity and inquisitivity. Start with wonderings and try to translate those questions and thoughts into code. This notebook will have an open end, allowing you to do some exploration of your own before moving on to the next step of dataset creation and feature engineering!\n",
    "\n",
    "Here are a couple, example questions you might form: What are the range of values that \"value\" can take? What information indicates when someone's device is plugged in? \n",
    "\n",
    "But before you ask questions of your own, let's start with Andy's:\n",
    "\n",
    ">**TASK(s)**: Using the data exploration and visualizations skills you've learned so far, answer the following questions. You may find it helpful to write down the answers in markdown cells or elsewhere so that you can create a data summary for your team, later on. \n",
    "\n",
    "1. How many user devices (id’s) are in this dataset? \n",
    "2. Between what range of dates was this data collected?\n",
    "3. Are there any null or missing values to deal with?\n",
    "4. How long do users typically charge their devices?\n",
    "5. What times of day are users most likely to plug in their devices and leave them plugged in for a long time (over 3hrs)? \n",
    "\n",
    "**References**: \n",
    "\n",
    "* Again, it may be very useful to reference the example EDA notebook provided, **EDA for Household Power**. \n",
    "* It also may be useful to look at **[pandas tutorials](https://pandas.pydata.org/docs/getting_started/index.html#intro-to-pandas)** which cover a wide range of topics, including: how to select and filter for specific rows of data,   and how to handle time series data (especially useful for this dataset!)\n",
    "* Finally, when in doubt, you can always search Stack Overflow or search for answers online with simple queries like, \"find unique values pandas\"\n",
    "\n",
    "Some questions can be answered with one line of code, while others may take multiple steps. \n",
    "\n",
    "The answer to question 5 is provided for you, as an example, and questions 1-4 are left for you to answer. **The provided example code assumes you have a correctly formatted DataFrame named `df` to work with.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example: Q5 \n",
    "\n",
    "**Q5: What times of day are users most likely to plug in their devices and leave them plugged in for a long time (over 3hrs)?**\n",
    "\n",
    "The first step is to turn this into a low-level question, where this is reframed in terms of the dataset we have. There are a few things this questions is asking about:\n",
    "* Looking at times when a user's device is plugged-in and charging, which is indicated by the `stream` column, \"/device/isPluggedIn\" taking on the `value` of 1\n",
    "* Select only those instances when a user has kept their device plugged in for _at least_ 3hrs, as indicated by `start` and `end` times of the plug-in event\n",
    "* Finally, of those long-charge events, find the most common start time, which I'll define as the hr at which someone `start`ed to plug-in their device for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q5 step 1: get rows where a device is plugged in and charging\n",
    "\n",
    "# selecting by stream and value\n",
    "plug_in_condition = (df['stream'] == \"/device/isPluggedIn\") & (df['value'] == 1.0)\n",
    "plugin_df = df[plug_in_condition].copy()\n",
    "\n",
    "## Q5 step 2: of plugin events, get events where end - start is >= 3hrs long\n",
    "\n",
    "# calculate the duration (in minutes) of a plugin event, and add it as a new column\n",
    "# dt.total_seconds converts an amount of time into seconds and then dividing by 60 gives minutes\n",
    "plugin_df['duration_mins'] = (plugin_df['end'] - plugin_df['start']).dt.total_seconds() / 60\n",
    "\n",
    "# selecting by >= 3hr durations\n",
    "long_charge_condition = plugin_df['duration_mins'] >= 60*3  # 3 hrs in minutes\n",
    "long_charges = plugin_df[long_charge_condition].copy()\n",
    "\n",
    "# check work so far, should see only long plugged-in events\n",
    "long_charges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating columns\n",
    "\n",
    "You'll notice that above, I created a column to record the duration of a plugged-in charge event. In general, creating columns that hold simple calculations or extracted information made from other variables in the dataset is a great way to explore and answer more complicated questions about this data. \n",
    "\n",
    "One useful thing to know about date-time stamps is that you can extract information like the hour, day, whether a time stamp falls on a weekend, and more with the use of date-time properties. \n",
    "\n",
    "I extract the hr (0-23) of the `start` times of long charge events, below, where 0 corresponds to midnight and 23 to 11pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q5 step 3: of the long charge events, find the most common start hr\n",
    "\n",
    "# first extract the starting hr and save in a column\n",
    "long_charges['start_hr'] = long_charges['start'].dt.hour\n",
    "\n",
    "## the most common start hr can then be described in a few ways, \n",
    "# by descriptive statistics like the mode (the value that appears most often)\n",
    "# or by visualizing the distribution of start times in a histogram\n",
    "\n",
    "# histogram: note that in the 0-23 hr range, 0 corresponds to midnight and 23 to 11pm\n",
    "# where do you notice spikes in long-charge start times?\n",
    "plt.hist(long_charges['start_hr'], bins=24, rwidth=0.9) # 24 bins for 24 hrs, 0.9 spacing for visual distinction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5 Answer**: It looks like the most common start hrs for people who charge their devices for a long time (>= 3hrs) are at night time, with the largest histogram peaks happening between hrs 22-23, and 0 or between 10pm and midnight, suggesting long, overnight charges!\n",
    "\n",
    "There is another smaller peak in the afternoon, around hr 15, so around 3 or 4pm, which is interesting.\n",
    "\n",
    "---\n",
    "\n",
    "Next, it is up to you to answer the rest of Andy's questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Answer Andy's questions (1-4) by using a variety of data exploration skills\n",
    "# Questions listed below for convenience:\n",
    "\n",
    "# Q1: How many user devices (id’s) are in this dataset?\n",
    "# Q2: Between what range of dates was this data collected?\n",
    "# Q3: Are there any null or missing values to deal with?\n",
    "# Q4: How long do users typically charge their devices?\n",
    "\n",
    "# Use Esc+B to create more code cells below this one, as you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Further exploration?\n",
    "\n",
    "What else can you find out about the data? Use the cells below to explore the data further, and add more cells as you need to.\n",
    "\n",
    ">**TASK**: Explore the data further.  Form and ask your own questions, be curious, try things out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room for exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Your Work!\n",
    "\n",
    "**Before you go!**\n",
    "\n",
    "After exploration, your next steps will be to create, specific features you'll use to train an ML model. If there are features you've created for this data that you would like to persist, **make sure you save your work**. \n",
    "\n",
    "> That means `Cmd+S` your current notebook AND save your current data, if you think it will be useful—for example, it will be useful to work with _only_ plugged-in, charging events and have a column that records the duration of those charges. I usually save my working data in a `data/` folder that I create in the same place that I work with notebooks. It's a good idea to give your data files descriptive names for reference.\n",
    "\n",
    "> Once you've saved your data, this would be a good point to **unmount** `umount()` the Trove data, if you are done working with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's suggested that you save in binary, pickle format because that will be much faster than, say, a CSV format, plus this format has the added bonus of saving any formatting changes you've done.\n",
    "\n",
    "If you need to change the file path in the commented code below, feel free to do so.  \n",
    "\n",
    "Later, we'll be using `read_pickle()` from the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html) to read in binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Save your data, if you have features you want to persist\n",
    "\n",
    "## (Optional) TODO: Unmount your trove data by URI if you are done working with it\n",
    "\n",
    "# uncomment and modify the code below to save your dataframe (e.g. plugin_df) to a specific directory (like data/)\n",
    "# plugin_df.to_pickle('~/data/plugin_events.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
